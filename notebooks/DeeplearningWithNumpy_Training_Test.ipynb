{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5dcd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST_Datset/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0b6fabfb54454bab9b552262207151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_Datset/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST_Datset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST_Datset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66660fedf364f60b6b6160662e23e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_Datset/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST_Datset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST_Datset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da93ec3bf8e643e9b5f76a873e6fcc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_Datset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST_Datset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST_Datset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60213b1e1aea4670a093af16a723b616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_Datset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST_Datset/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kji/anaconda3/envs/py39_0/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "\n",
    "download_path = \"./MNIST_Datset\"\n",
    "train_dataset = MNIST(download_path, train=True, download=True)\n",
    "valid_dataset = MNIST(download_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac65db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward(x, layers):\n",
    "    out = x\n",
    "    for idx, layer in enumerate(layers):\n",
    "        out = layer.forward(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _backward(back_propagation, layers):\n",
    "    back = back_propagation\n",
    "    for layer in reversed(layers):\n",
    "        back = layer.backward(back)\n",
    "\n",
    "def one_hot(y):\n",
    "    n = len(y)\n",
    "\n",
    "    one_hot_y = np.zeros((n, 10))\n",
    "    idx = [i for i in range(n)]\n",
    "    one_hot_y[idx, y] = 1\n",
    "    return one_hot_y\n",
    "\n",
    "def MSE_Loss(output, y):\n",
    "    return np.sum(((output - y) ** 2) / 2)\n",
    "\n",
    "def Cross_Entropy_Loss(output, y):\n",
    "    return np.sum(-(y * np.log(output)))\n",
    "\n",
    "def softmax(output):\n",
    "    exp_output = np.exp(output)\n",
    "    exp_sum = np.sum(exp_output, axis=1)\n",
    "    return exp_output / exp_sum[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a093eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer_np():\n",
    "    '''\n",
    "        들어온 신호가 있는 pixel만 backprop를 전달함\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._prev_input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._prev_input = np.array(x, copy=True)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, _back_gradient):\n",
    "        mask = self._prev_input > 0\n",
    "        _back = _back_gradient * mask\n",
    "        return _back\n",
    "\n",
    "    def get_weight(self):\n",
    "        return None\n",
    "\n",
    "class LinearLayer_np():\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self._linear_weight = np.random.uniform(low=(-np.sqrt(6/input_dim)),high=np.sqrt(6/input_dim),size=(input_dim, output_dim)) #np.random.randn(input_dim, output_dim) * 0.1\n",
    "        self._linear_bias = np.random.randn(output_dim) * 0.01\n",
    "\n",
    "        self._prev_input = None\n",
    "        self._update_w = None\n",
    "        self._update_b = None\n",
    "\n",
    "    def get_weight(self):\n",
    "        return self._update_w, self._linear_weight, self._update_b, self._linear_bias\n",
    "\n",
    "    def set_weight(self, w, b):\n",
    "        self._linear_weight = w\n",
    "        self._linear_bias = b\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._prev_input = np.array(x, copy=True)\n",
    "\n",
    "        return np.dot(x, self._linear_weight) + self._linear_bias\n",
    "\n",
    "\n",
    "    def backward(self, _back_gradient):\n",
    "        output = np.zeros((self._prev_input.shape))\n",
    "\n",
    "        self._update_w = np.zeros((self._linear_weight.shape))\n",
    "        self._update_b = np.zeros((self._linear_bias.shape))\n",
    "        self._update_w += np.mean(self._prev_input[:, :, np.newaxis] * _back_gradient[:, np.newaxis, :], axis=0)\n",
    "\n",
    "        self._update_b += np.mean(_back_gradient, axis=0)\n",
    "        output += np.sum(self._linear_weight[np.newaxis, :, :] * _back_gradient[:, np.newaxis, :], axis=-1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MaxPoolLayer_np():\n",
    "    def __init__(self, kernel_size=2, stride=1, padding=0):\n",
    "\n",
    "        self._prev_input = None\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self._mask_coord = dict()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._prev_input = np.array(x, copy=True)\n",
    "        N, H, W, C = x.shape\n",
    "\n",
    "        padding_x = np.zeros((N, H + 2 * self.padding, W + 2 * self.padding, C))\n",
    "        padding_x[:, self.padding:(self.padding + H), self.padding:(self.padding + W), :] = x\n",
    "\n",
    "        self.h_stride = (H+2*self.padding - self.kernel_size) // self.stride + 1\n",
    "        self.w_stride = (W+2*self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((N, self.h_stride, self.w_stride, C))\n",
    "        self.max_output_mask = np.zeros_like(padding_x)\n",
    "\n",
    "        for j in range(self.h_stride):\n",
    "            start_j_index = j * self.stride\n",
    "            end_j_index = j * self.stride + self.kernel_size\n",
    "            for i in range(self.w_stride):\n",
    "                start_i_index = i * self.stride\n",
    "                end_i_index = i * self.stride + self.kernel_size\n",
    "                mask = padding_x[:, start_j_index:end_j_index, start_i_index:end_i_index, :]\n",
    "                max_value = np.max(mask, axis=(1, 2))\n",
    "                output[:, j, i, :] = max_value\n",
    "                self.max_output_mask[:, start_j_index:end_j_index, start_i_index:end_i_index, :] = max_value[:, np.newaxis, np.newaxis, :]\n",
    "                self._mask_coord[(j, i)] = np.array(self.max_output_mask[:, start_j_index:end_j_index, start_i_index:end_i_index, :], copy=True)\n",
    "        return output\n",
    "\n",
    "    def backward(self, _back_gradient):\n",
    "        output = np.zeros((self._prev_input.shape))\n",
    "        for j in range(self.h_stride):\n",
    "            start_j_index = j * self.stride\n",
    "            end_j_index = j * self.stride + self.kernel_size\n",
    "            for i in range(self.w_stride):\n",
    "                start_i_index = i * self.stride\n",
    "                end_i_index = i * self.stride + self.kernel_size\n",
    "                mask = self._mask_coord[(j, i)]\n",
    "                temp = self._prev_input[:, start_j_index:end_j_index, start_i_index:end_i_index, :] >= mask\n",
    "                order_mask = np.arange(1, self.kernel_size * self.kernel_size + 1).reshape(self.kernel_size, self.kernel_size)[np.newaxis, :, :, np.newaxis]\n",
    "                temp_mask = order_mask * temp\n",
    "                max_value = np.max(temp_mask, axis=(1, 2))\n",
    "                ori_temp_mask = np.array(temp_mask, copy=True)\n",
    "                temp_mask[:, :, :, :] = max_value[:, np.newaxis, np.newaxis, :]\n",
    "                real_mask = temp_mask <= ori_temp_mask\n",
    "                output[:, start_j_index:end_j_index, start_i_index:end_i_index, :] += mask * real_mask\n",
    "        return output\n",
    "\n",
    "    def get_weight(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "class FlattenLayer_np():\n",
    "    # input N H W C\n",
    "    # output N x (H * W * C)\n",
    "    def __init__(self):\n",
    "        self._prev_input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, H, W, C = x.shape\n",
    "        self._prev_input = np.array(x, copy=True)\n",
    "        output = x.reshape(N, H * W * C)\n",
    "        return output\n",
    "\n",
    "    def backward(self, _back_gradient):\n",
    "        N, H, W, C = self._prev_input.shape\n",
    "        return _back_gradient.reshape(N, H, W, C)\n",
    "\n",
    "    def get_weight(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "class Adam():\n",
    "    def __init__(self, beta1=0.9, beta2=0.999, lr=0.001, eps=1e-8):\n",
    "        self.step = 1\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "\n",
    "        self._m = dict()\n",
    "        self._v = dict()\n",
    "\n",
    "\n",
    "    def update(self, layers):\n",
    "        if len(self._m) == 0 or len(self._v) == 0:\n",
    "            self._init_mv(layers)\n",
    "\n",
    "        for idx, layer in enumerate(layers):\n",
    "            gradient = layer.get_weight\n",
    "            if gradient() is None:\n",
    "                continue\n",
    "            dw, w, db, b = gradient()\n",
    "            dw_key = f\"dw{idx}\"\n",
    "            db_key = f\"db{idx}\"\n",
    "            self._m[dw_key] = self.beta1 * self._m[dw_key] + (1 - self.beta1) * dw\n",
    "            self._m[db_key] = self.beta1 * self._m[db_key] + (1 - self.beta1) * db\n",
    "\n",
    "            self._v[dw_key] = self.beta2 * self._v[dw_key] + (1 - self.beta2) * (dw ** 2)\n",
    "            self._v[db_key] = self.beta2 * self._v[db_key] + (1 - self.beta2) * (db ** 2)\n",
    "\n",
    "            bias_correction1_w = self._m[dw_key] / (1 - (self.beta1 ** self.step))\n",
    "            bias_correction1_b = self._m[db_key] / (1 - (self.beta1 ** self.step))\n",
    "\n",
    "            bias_correction2_w = self._v[dw_key] / (1 - (self.beta2 ** self.step))\n",
    "            bias_correction2_b = self._v[db_key] / (1 - (self.beta2 ** self.step))\n",
    "\n",
    "            next_w = w - self.lr * bias_correction1_w / (np.sqrt(bias_correction2_w) + self.eps)\n",
    "            next_b = b - self.lr * bias_correction1_b / (np.sqrt(bias_correction2_b) + self.eps)\n",
    "\n",
    "            layer.set_weight(w=next_w, b=next_b)\n",
    "\n",
    "\n",
    "    def _init_mv(self, layers):\n",
    "        for idx, layer in enumerate(layers):\n",
    "\n",
    "            gradient = layer.get_weight\n",
    "            if gradient() is None:\n",
    "                continue\n",
    "\n",
    "            dw, w, db, b = gradient()\n",
    "            dw_key = f\"dw{idx}\"\n",
    "            db_key = f\"db{idx}\"\n",
    "\n",
    "            self._m[dw_key] = np.zeros_like(dw)\n",
    "            self._m[db_key] = np.zeros_like(db)\n",
    "\n",
    "            self._v[dw_key] = np.zeros_like(dw)\n",
    "            self._v[db_key] = np.zeros_like(db)\n",
    "\n",
    "class ConvLayer_np():\n",
    "    '''\n",
    "        input : N x C(input_dim) x H x W\n",
    "        output : N x C(output_dim) x h x w\n",
    "\n",
    "        kernel_weight : input_dim x output_dim x kernel_size x kernel_size\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, stride, padding):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self._init_weight()\n",
    "\n",
    "        self._prev_input = None\n",
    "        self._update_w = None\n",
    "        self._update_b = None\n",
    "\n",
    "    def get_weight(self):\n",
    "        return self._update_w, self.kernel_weight, self._update_b, self.kernel_bias\n",
    "\n",
    "    def set_weight(self, w, b):\n",
    "        self.kernel_weight = w\n",
    "        self.kernel_bias = b\n",
    "\n",
    "    def _init_weight(self):\n",
    "        self.kernel_weight = np.random.uniform(low=(-np.sqrt(6/self.input_dim)),\n",
    "                                               high=np.sqrt(6/self.input_dim),\n",
    "                                               size=(self.kernel_size, self.kernel_size, self.input_dim, self.output_dim))\n",
    "        # self.kernel_weight = np.random.randn(self.kernel_size, self.kernel_size,  self.input_dim, self.output_dim) * 0.1\n",
    "        self.kernel_bias = np.random.randn(self.output_dim) * 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, H, W, C = x.shape\n",
    "        self._prev_input = np.array(x, copy=True) #deep copy\n",
    "\n",
    "        w_stride = (W + self.padding * 2 - self.kernel_size) // self.stride + 1\n",
    "        h_stride = (H + self.padding * 2 - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((N,  h_stride, w_stride, self.output_dim))\n",
    "        for idx in range(N):\n",
    "            for h_i in range(h_stride):\n",
    "                for w_i in range(w_stride):\n",
    "                    padding_x = np.zeros((H + self.padding * 2, W + self.padding * 2, C))\n",
    "                    padding_x[self.padding:self.padding+H, self.padding:self.padding+W, :] = x[idx, :, :]\n",
    "                    conv_img = padding_x[ h_i * self.stride:(h_i * self.stride + self.kernel_size), w_i*self.stride:(w_i * self.stride + self.kernel_size), :]\n",
    "                    for out_dim_i in range(self.output_dim):\n",
    "                        conv_kernel = self.kernel_weight[:, :, :, out_dim_i]\n",
    "                        _conv = conv_img * conv_kernel\n",
    "                        _conv_weight = np.sum(_conv)\n",
    "                        output[idx, h_i, w_i, out_dim_i] = _conv_weight\n",
    "\n",
    "        return output + self.kernel_bias\n",
    "\n",
    "    def backward(self, _back_gradient):\n",
    "        # 들어온 gradient에 현재 픽셀에 관련된 값을 다음 gradient에 더함\n",
    "        # batch size만큼 평균을 내줌\n",
    "\n",
    "        # backpropagation gradient\n",
    "        N, H, W, C = self._prev_input.shape\n",
    "        output = np.zeros((N, H+self.padding*2, W+self.padding*2, C))\n",
    "        padding_prev_input = np.zeros(output.shape)\n",
    "        padding_prev_input[:, self.padding:self.padding+H, self.padding:self.padding+W, :] = self._prev_input[:, :, :, :]\n",
    "        self._update_w = np.zeros(self.kernel_weight.shape)\n",
    "        self._update_b = np.zeros(self.kernel_bias.shape)\n",
    "        '''\n",
    "            커널에 backgradient를 곱한것을 뒤로 전달\n",
    "        '''\n",
    "        batch_n, height_out, width_out, out_dim = _back_gradient.shape\n",
    "        weight_h, weight_w, input_dim, output_dim = self.kernel_weight.shape\n",
    "\n",
    "        for col_i in range(width_out): # column\n",
    "            output_i_start = col_i * self.stride\n",
    "            output_i_end = output_i_start + weight_w\n",
    "            for row_j in range(height_out): # row\n",
    "                output_j_start = row_j * self.stride\n",
    "                output_j_end = output_j_start + weight_h\n",
    "\n",
    "                for dim_k in range(out_dim):\n",
    "                    # output dim [256,3,3,2]\n",
    "                    output[:, output_i_start:output_i_end, output_j_start:output_j_end, :] += (_back_gradient[:, np.newaxis, col_i:col_i+1, row_j:row_j+1, dim_k:dim_k+1] * \\\n",
    "                                                                                           self.kernel_weight[np.newaxis, :, :, :, dim_k:dim_k+1]).squeeze(axis=-1) # [256,1(new),1,1,1] * [1(new),3,3,2,1] => [256,3,3,2,1]\n",
    "\n",
    "                    # update_w dim [3,3,2,4] / back_gradient dim [256,3,3,4] / _prev_input dim [256, 12, 12, 2] => [256, 3, 3, 2]\n",
    "                    self._update_w[:, :, :, dim_k:dim_k+1] += (_back_gradient[:, col_i:col_i+1, row_j:row_j+1, np.newaxis, dim_k:dim_k+1]*\n",
    "                                                       padding_prev_input[:, output_i_start:output_i_end, output_j_start:output_j_end, :, np.newaxis]).sum(axis=0) / batch_n\n",
    "                    # [3,3,2,4] += [256,3,3,1(new),4] * [256,3,3,2,1(new)] => [256,3,3,2,4]\n",
    "\n",
    "                    self._update_b[dim_k] += np.sum(_back_gradient[:, col_i:col_i+1, row_j:row_j+1, np.newaxis, dim_k:dim_k+1]) / batch_n\n",
    "        return output\n",
    "\n",
    "class LogSoftMax_np():\n",
    "    def __init__(self):\n",
    "        self._prev_input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : N x class_num\n",
    "\n",
    "        self._prev_input = np.array(x, copy=True)\n",
    "        _exp_input = np.exp(x)\n",
    "        _exp_sum = np.sum(_exp_input, axis=1)[:, np.newaxis]\n",
    "\n",
    "        output = _exp_input / _exp_sum\n",
    "        # output = np.log(output)\n",
    "        # self._output = np.array(output, copy=True)\n",
    "        return output\n",
    "\n",
    "    def backward(self, _back_gradient):\n",
    "        return _back_gradient\n",
    "\n",
    "    def get_weight(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9039ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    # 28 x 28 x 1(Dim)\n",
    "    ConvLayer_np(input_dim=1, output_dim=16, kernel_size=5, stride=1, padding=2),\n",
    "    ReLULayer_np(),\n",
    "    # 28 x 28 x 8\n",
    "    MaxPoolLayer_np(kernel_size=2, stride=2, padding=0),\n",
    "    # 14 x 14 x 8\n",
    "    ConvLayer_np(input_dim=16, output_dim=32, kernel_size=3, stride=1, padding=0),\n",
    "    ReLULayer_np(),\n",
    "    # 12 x 12 x 8\n",
    "    MaxPoolLayer_np(kernel_size=2, stride=2, padding=0),\n",
    "    # 6 x 6 x 8\n",
    "    FlattenLayer_np(),\n",
    "    LinearLayer_np(input_dim=1152, output_dim=256),\n",
    "    ReLULayer_np(),\n",
    "    LinearLayer_np(input_dim=256, output_dim=10),\n",
    "    LogSoftMax_np()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6ea0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.0001)\n",
    "LossFunc = None\n",
    "\n",
    "trainX = train_dataset.data.numpy()\n",
    "trainY = train_dataset.targets.numpy()\n",
    "\n",
    "testX = valid_dataset.data.numpy()\n",
    "testY = valid_dataset.targets.numpy()\n",
    "\n",
    "total_epoch = 20\n",
    "batch_size = 100\n",
    "\n",
    "trainX = trainX[:60000] / 255.0 \n",
    "trainY = trainY[:60000]\n",
    "\n",
    "testX = testX[:10000]\n",
    "testY = testY[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b519dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0: 100%|██████████████████████████████████████████████████| 600/600 [2:13:37<00:00, 13.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 / loss:63.33228858164571 / Acc:91.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████████████████████████████████████████████| 600/600 [2:08:28<00:00, 12.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 / loss:25.012045140303083 / Acc:93.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2: 100%|██████████████████████████████████████████████████| 600/600 [1:45:56<00:00, 10.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 / loss:20.828275472384306 / Acc:94.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3: 100%|██████████████████████████████████████████████████| 600/600 [1:47:27<00:00, 10.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 / loss:18.80353689706 / Acc:94.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4: 100%|██████████████████████████████████████████████████| 600/600 [1:46:28<00:00, 10.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 / loss:17.656414802759347 / Acc:94.89999999999999%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5: 100%|██████████████████████████████████████████████████| 600/600 [1:48:33<00:00, 10.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 / loss:16.967753436408792 / Acc:95.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6: 100%|██████████████████████████████████████████████████| 600/600 [1:45:06<00:00, 10.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 / loss:16.55541912486767 / Acc:95.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7: 100%|██████████████████████████████████████████████████| 600/600 [1:44:46<00:00, 10.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 / loss:16.35387544988638 / Acc:95.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8: 100%|██████████████████████████████████████████████████| 600/600 [1:37:04<00:00,  9.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 / loss:16.361360650187343 / Acc:95.67999999999999%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9: 100%|██████████████████████████████████████████████████| 600/600 [1:39:56<00:00,  9.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 / loss:16.593731036675795 / Acc:95.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10: 100%|█████████████████████████████████████████████████| 600/600 [1:40:14<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 / loss:17.052510929713826 / Acc:95.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11: 100%|█████████████████████████████████████████████████| 600/600 [1:45:57<00:00, 10.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 / loss:17.73684521510844 / Acc:95.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 12: 100%|█████████████████████████████████████████████████| 600/600 [1:46:31<00:00, 10.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 / loss:18.661012213820477 / Acc:95.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 13: 100%|█████████████████████████████████████████████████| 600/600 [1:45:19<00:00, 10.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 / loss:19.796590694821546 / Acc:95.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 14: 100%|█████████████████████████████████████████████████| 600/600 [1:39:34<00:00,  9.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 / loss:21.14038522085117 / Acc:94.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 15: 100%|█████████████████████████████████████████████████| 600/600 [1:54:47<00:00, 11.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 / loss:22.714402585108903 / Acc:94.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16: 100%|█████████████████████████████████████████████████| 600/600 [2:07:26<00:00, 12.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 / loss:24.516768552776213 / Acc:93.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 17: 100%|█████████████████████████████████████████████████| 600/600 [2:08:54<00:00, 12.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 / loss:26.54560838901244 / Acc:93.30000000000001%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 18: 100%|█████████████████████████████████████████████████| 600/600 [2:10:07<00:00, 13.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 / loss:28.79938493905757 / Acc:92.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 19: 100%|█████████████████████████████████████████████████| 600/600 [2:08:54<00:00, 12.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 / loss:31.295395751504195 / Acc:92.4%\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    step = int(len(trainX) / batch_size)\n",
    "    loss_list = []\n",
    "    for step_i in tqdm.tqdm(range(step), ncols=100, desc=f\"epoch {epoch}\"):\n",
    "\n",
    "        X = trainX[step_i*batch_size:(step_i+1)*batch_size]\n",
    "        Y = trainY[step_i*batch_size:(step_i+1)*batch_size]\n",
    "        one_hot_Y = one_hot(Y)\n",
    "        X = X[:, :, :, np.newaxis]\n",
    "        output = _forward(X, layers)\n",
    "        back_propagation = output - one_hot_Y\n",
    "        # loss = MSE_Loss(output, one_hot_Y)\n",
    "        loss = Cross_Entropy_Loss(output, one_hot_Y)\n",
    "        loss_list.append(loss)\n",
    "        # print(loss)\n",
    "        _backward(back_propagation, layers)\n",
    "\n",
    "        optimizer.update(layers=layers)\n",
    "\n",
    "    TEST_X = testX[:, :, :, np.newaxis] / 255.0\n",
    "    # TEST_Y = one_hot(testY)\n",
    "\n",
    "    output = _forward(TEST_X, layers)\n",
    "    # softmax_output = softmax(output)\n",
    "    acc = np.mean(testY == np.argmax(output, axis=1))\n",
    "    Text = f'epoch:{epoch} / loss:{np.mean(loss_list)} / Acc:{acc*100.0}%'\n",
    "    print(f'epoch:{epoch} / loss:{np.mean(loss_list)} / Acc:{acc*100.0}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e83ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c90ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
